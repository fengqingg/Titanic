{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Titanic \nHello and welcome to my code for the Titanic dataset from Kaggle! This code is aimed at exploring the dataset and making predictions about the survival of the passengers using machine learning techniques.\n\nIn this code, I have used Python and popular libraries such as Pandas, NumPy, and Scikit-Learn to preprocess the data, perform feature engineering, and build a predictive model. The dataset contains information about the passengers aboard the Titanic, including their age, gender, ticket class, fare, and survival status.\n\nAfter cleaning and exploring the data, I have used various machine learning algorithms, such as Logistic Regression, Decision Trees, and Random Forests, to build and evaluate predictive models.\n\n\n## Table of Contents\n\n* [Introduction](#section-one)\n    - [Loading of dataset](#subsection-one)\n* [Exploratory Data Analysis](#section-two)\n    - [Exploration of data](#subsection-two)\n    - [Analysis of data](#subsection-three)\n    - [Transformation of data](#subsection-four)\n* [Modelling and predicting](#section-three)\n    - [Cross validation and Gridsearch](#subsection-five)\n* [Submission](#section-four)\n* [Conclusion](#section-five)","metadata":{}},{"cell_type":"markdown","source":"<a id=\"section-one\"></a>\n# Introduction\nThe Titanic dataset available on Kaggle is a classic binary classification problem. The aim is to predict whether a passenger aboard the Titanic survived or not based on various attributes such as their age, gender, ticket class, etc.\n\nThe Titanic dataset contains information on 891 passengers out of the 2224 people on board the Titanic. Each passenger is described by the following features:\n\n**PassengerId**: A unique identifier for each passenger.\\\n**Survived**: A binary variable indicating whether the passenger survived or not (0 = No, 1 = Yes).\\\n**Pclass**: The ticket class of the passenger (1 = 1st class, 2 = 2nd class, 3 = 3rd class).\\\n**Name**: The name of the passenger.\\\n**Sex**: The gender of the passenger.\\\n**Age**: The age of the passenger in years.\\\n**SibSp**: The number of siblings/spouses aboard the Titanic.\\\n**Parch**: The number of parents/children aboard the Titanic.\\\n**Ticket**: The ticket number of the passenger.\\\n**Fare**: The fare paid by the passenger.\\\n**Cabin**: The cabin number of the passenger.\\\n**Embarked**: The port of embarkation (C = Cherbourg, Q =\nQueenstown, S = Southampton).\n","metadata":{}},{"cell_type":"markdown","source":"<a id=\"subsection-one\"></a>\n### Load data and importing libraries","metadata":{"execution":{"iopub.status.busy":"2023-03-27T06:52:40.026658Z","iopub.execute_input":"2023-03-27T06:52:40.027122Z","iopub.status.idle":"2023-03-27T06:52:40.035010Z","shell.execute_reply.started":"2023-03-27T06:52:40.027085Z","shell.execute_reply":"2023-03-27T06:52:40.033241Z"}}},{"cell_type":"code","source":"# Data analysis \nimport pandas as pd\nimport numpy as np\nimport random as rnd\n\n# Visualization of data\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Machine learning\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import KFold","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-04-04T06:40:47.297084Z","iopub.execute_input":"2023-04-04T06:40:47.297519Z","iopub.status.idle":"2023-04-04T06:40:47.313668Z","shell.execute_reply.started":"2023-04-04T06:40:47.297481Z","shell.execute_reply":"2023-04-04T06:40:47.311127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Loading train data and test data\ntrain_df = pd.read_csv('/kaggle/input/titanic/train.csv')\ntest_df = pd.read_csv('/kaggle/input/titanic/test.csv')","metadata":{"execution":{"iopub.status.busy":"2023-04-04T06:40:47.316025Z","iopub.execute_input":"2023-04-04T06:40:47.317063Z","iopub.status.idle":"2023-04-04T06:40:47.349882Z","shell.execute_reply.started":"2023-04-04T06:40:47.317011Z","shell.execute_reply":"2023-04-04T06:40:47.348845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"section-two\"></a>\n## Exploratory Data Analysis (EDA)","metadata":{}},{"cell_type":"markdown","source":"<a id=\"subsection-two\"></a>\n\n### Exploration of data\nLet's take a look at the train_data to understand more about it.","metadata":{}},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-04-04T06:40:47.351183Z","iopub.execute_input":"2023-04-04T06:40:47.352297Z","iopub.status.idle":"2023-04-04T06:40:47.379193Z","shell.execute_reply.started":"2023-04-04T06:40:47.352252Z","shell.execute_reply":"2023-04-04T06:40:47.377146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.info()","metadata":{"execution":{"iopub.status.busy":"2023-04-04T06:40:47.381953Z","iopub.execute_input":"2023-04-04T06:40:47.383382Z","iopub.status.idle":"2023-04-04T06:40:47.401567Z","shell.execute_reply.started":"2023-04-04T06:40:47.383314Z","shell.execute_reply":"2023-04-04T06:40:47.400017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.info()","metadata":{"execution":{"iopub.status.busy":"2023-04-04T06:40:47.404717Z","iopub.execute_input":"2023-04-04T06:40:47.405229Z","iopub.status.idle":"2023-04-04T06:40:47.428099Z","shell.execute_reply.started":"2023-04-04T06:40:47.405182Z","shell.execute_reply":"2023-04-04T06:40:47.425880Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Look for missing data\ntrain_df.isnull().sum() #checking for total null values","metadata":{"execution":{"iopub.status.busy":"2023-04-04T06:40:47.429874Z","iopub.execute_input":"2023-04-04T06:40:47.430344Z","iopub.status.idle":"2023-04-04T06:40:47.441513Z","shell.execute_reply.started":"2023-04-04T06:40:47.430297Z","shell.execute_reply":"2023-04-04T06:40:47.439955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2023-04-04T06:40:47.442925Z","iopub.execute_input":"2023-04-04T06:40:47.443859Z","iopub.status.idle":"2023-04-04T06:40:47.455183Z","shell.execute_reply.started":"2023-04-04T06:40:47.443812Z","shell.execute_reply":"2023-04-04T06:40:47.453890Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the train data, 'Age', 'Cabin', 'Embarked' contains null values and it is important to deal with it before selecting features. \n\nIt is also noted that the test data might contain null values and have to handle it before predicting.","metadata":{}},{"cell_type":"code","source":"# Get the count of passengers who survived \nsurvive = train_df.Survived.value_counts()\n\n# Create a pie chart with the survival counts \nplt.pie(survive, labels=survive.index, autopct='%1.1f%%')\n\n# Set the title of the chart\nplt.title(\"Survived\")\n\n# Add a legend to the chart\nplt.legend()\n\n# Show the chart\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-04-04T06:40:47.456552Z","iopub.execute_input":"2023-04-04T06:40:47.457479Z","iopub.status.idle":"2023-04-04T06:40:47.670675Z","shell.execute_reply.started":"2023-04-04T06:40:47.457425Z","shell.execute_reply":"2023-04-04T06:40:47.668588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Only 38.4% of the passengers survived. To find out how the features are related to the survival of the passengers, we will start to do some analysis.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"subsection-three\"></a>\n\n### Analysis of data\n\nAfter conducting our exploratory data analysis, we have identified several features that warrant further investigation. These features include Pclass, Sex, Age, SibSp, Parch, Ticket, Fare, Cabin, and Embarked.","metadata":{}},{"cell_type":"markdown","source":"#### PassengerId\n\nThe PassengerId is the unique identifier for each passenger which has no relation to whether the passenger survives. Thus this column will dropped.","metadata":{}},{"cell_type":"markdown","source":"#### Pclass","metadata":{}},{"cell_type":"code","source":"# Group the data by sex and survival status\ncounts = train_df.groupby(['Pclass', 'Survived']).size().unstack()\n\n# Create a stacked bar chart showing the counts of males and females by survival status\nax = counts.plot(kind='bar', stacked=True)\n\n# Set the title and axis labels\nplt.title('Pclass by Survival Status')\nplt.xlabel('Pclass')\nplt.xticks(rotation=0)\nplt.ylabel('Count')\n\n# Add labels to the top of each bar showing the count\nfor rect in ax.containers:\n    ax.bar_label(rect, labels=[f'{int(h)}' for h in rect.datavalues], label_type='edge')\n\n# Add a legend\nax.legend(title='Survived', loc='upper left')\nplt.show()\n\ntrain_df[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean().sort_values(by='Survived', ascending=False)","metadata":{"execution":{"iopub.status.busy":"2023-04-04T06:40:47.677157Z","iopub.execute_input":"2023-04-04T06:40:47.679032Z","iopub.status.idle":"2023-04-04T06:40:47.954699Z","shell.execute_reply.started":"2023-04-04T06:40:47.678908Z","shell.execute_reply":"2023-04-04T06:40:47.953620Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It is clear that the Pclass is an important feature to predict who survive as the survival rate of Pclass 3 is so much lower than the other classes. It can be due to the priority given to them or the location of their cabin.","metadata":{}},{"cell_type":"markdown","source":"#### Name\n\nLooking at the head of Name, it seems like this feature cannot be used as a feature to predict survivability of the passengers. This would be dropped when transforming the data.","metadata":{}},{"cell_type":"markdown","source":"#### Sex","metadata":{}},{"cell_type":"code","source":"# Group the data by sex and survival status\ncounts = train_df.groupby(['Sex', 'Survived']).size().unstack()\n\n# Create a stacked bar chart showing the counts of males and females by survival status\nax = counts.plot(kind='bar', stacked=True)\n\n# Set the title and axis labels\nplt.title('Males and Females by Survival Status')\nplt.xlabel('Sex')\nplt.xticks(rotation=0)\nplt.ylabel('Count')\n\n# Add labels to the top of each bar showing the count\nfor rect in ax.containers:\n    ax.bar_label(rect, labels=[f'{int(h)}' for h in rect.datavalues], label_type='edge')\n\n# Add a legend\nax.legend(title='Survived', loc='upper left')\nplt.show()\n\n# Group by Sex to get the mean of survival rate\ntrain_df[[\"Sex\", \"Survived\"]].groupby(['Sex'], as_index=False).mean().sort_values(by='Survived', ascending=False)","metadata":{"execution":{"iopub.status.busy":"2023-04-04T06:40:47.955914Z","iopub.execute_input":"2023-04-04T06:40:47.956739Z","iopub.status.idle":"2023-04-04T06:40:48.233478Z","shell.execute_reply.started":"2023-04-04T06:40:47.956701Z","shell.execute_reply":"2023-04-04T06:40:48.232216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Base on the data, it can be established that the female rate of survival is a lot higher than male. Thus sex is also an important feature to select.","metadata":{}},{"cell_type":"markdown","source":"#### Age","metadata":{}},{"cell_type":"markdown","source":"From the EDA, we saw that our train_df age has 177 missing data while test_df has 86 missing data. We can choose to ignore it or fill it in with a data, either imputing it with the median or the mean. However, as the passenger name salutation gives us a hint on what their age roughly should be, we can use it to have a better imputation. ","metadata":{}},{"cell_type":"code","source":"# Extracting the salutations from the name\ntrain_df['Initial']=0\nfor i in train_df:\n    train_df['Initial']=train_df.Name.str.extract('([A-Za-z]+)\\.')\n\n# Checking the salutations with the sex\npd.crosstab(train_df.Initial,train_df.Sex).T.style.background_gradient()","metadata":{"execution":{"iopub.status.busy":"2023-04-04T06:40:48.235080Z","iopub.execute_input":"2023-04-04T06:40:48.236133Z","iopub.status.idle":"2023-04-04T06:40:48.330530Z","shell.execute_reply.started":"2023-04-04T06:40:48.236091Z","shell.execute_reply":"2023-04-04T06:40:48.329252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The initial has high-cardinality at this moment, so it is best to replace initials with similar group.","metadata":{}},{"cell_type":"code","source":"# Changing the initials of train_df\ntrain_df['Initial'].replace(['Mlle','Mme','Ms','Dr','Major','Lady','Countess','Jonkheer','Col','Rev','Capt','Sir','Don'],['Miss','Miss','Miss','Mr','Mr','Mrs','Mrs','Other','Other','Other','Mr','Mr','Mr'],inplace=True)\n# Checking the mean age of the intials\ntrain_df.groupby('Initial').Age.mean()","metadata":{"execution":{"iopub.status.busy":"2023-04-04T06:40:48.332671Z","iopub.execute_input":"2023-04-04T06:40:48.333201Z","iopub.status.idle":"2023-04-04T06:40:48.349530Z","shell.execute_reply.started":"2023-04-04T06:40:48.333148Z","shell.execute_reply":"2023-04-04T06:40:48.348171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Imputing the NaN age with respective salutation mean age\nage_mapping = {'Mr': 33, 'Mrs': 36, 'Master': 5, 'Miss': 22, 'Other': 46}\ntrain_df['Age'] = train_df['Age'].fillna(train_df['Initial'].map(age_mapping))","metadata":{"execution":{"iopub.status.busy":"2023-04-04T06:40:48.351686Z","iopub.execute_input":"2023-04-04T06:40:48.352234Z","iopub.status.idle":"2023-04-04T06:40:48.362548Z","shell.execute_reply.started":"2023-04-04T06:40:48.352183Z","shell.execute_reply":"2023-04-04T06:40:48.360958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Extracting the salutations from the name of test_df\ntest_df['Initial']=0\nfor i in test_df:\n    test_df['Initial']=test_df.Name.str.extract('([A-Za-z]+)\\.')\n\n# Changing the initials of test_df\ntest_df['Initial'].replace(['Mlle','Mme','Ms','Dr','Major','Lady','Countess','Jonkheer','Col','Rev','Capt','Sir','Don'],['Miss','Miss','Miss','Mr','Mr','Mrs','Mrs','Other','Other','Other','Mr','Mr','Mr'],inplace=True)\n# Checking the mean age of the intials\ntest_df.groupby('Initial').Age.mean()","metadata":{"execution":{"iopub.status.busy":"2023-04-04T06:40:48.364447Z","iopub.execute_input":"2023-04-04T06:40:48.365165Z","iopub.status.idle":"2023-04-04T06:40:48.414011Z","shell.execute_reply.started":"2023-04-04T06:40:48.365106Z","shell.execute_reply":"2023-04-04T06:40:48.412476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Imputing the NaN age with respective salutation mean age\nage_mapping = {'Mr': 32, 'Mrs': 39, 'Master': 7, 'Miss': 22, 'Other': 43}\ntest_df['Age'] = test_df['Age'].fillna(test_df['Initial'].map(age_mapping))","metadata":{"execution":{"iopub.status.busy":"2023-04-04T06:40:48.415922Z","iopub.execute_input":"2023-04-04T06:40:48.416398Z","iopub.status.idle":"2023-04-04T06:40:48.427719Z","shell.execute_reply.started":"2023-04-04T06:40:48.416333Z","shell.execute_reply":"2023-04-04T06:40:48.426183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Subplot the histogram and KDE of age \nfig,ax=plt.subplots(1,2,figsize=(20,8))\nsns.kdeplot(data=train_df,x='Age',hue='Survived',fill=True, ax=ax[0])\nsns.histplot(data=train_df,x='Age',hue='Survived', alpha=0.4 ,ax=ax[1])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-04-04T06:40:48.429281Z","iopub.execute_input":"2023-04-04T06:40:48.429657Z","iopub.status.idle":"2023-04-04T06:40:49.101090Z","shell.execute_reply.started":"2023-04-04T06:40:48.429619Z","shell.execute_reply":"2023-04-04T06:40:49.099885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the plots, it seems like most of the passengers of age 20-40 has lower survival rate compared to the other age group. Also, passengers from age 0-10 has higher survival rate, thus age is also most likely a important feature to be selected.","metadata":{}},{"cell_type":"markdown","source":"#### SibSp","metadata":{}},{"cell_type":"code","source":"# Plot barchart of SibSp vs Survived\nsns.barplot(data=train_df, x='SibSp',y='Survived')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-04-04T06:40:49.103369Z","iopub.execute_input":"2023-04-04T06:40:49.104360Z","iopub.status.idle":"2023-04-04T06:40:49.550685Z","shell.execute_reply.started":"2023-04-04T06:40:49.104296Z","shell.execute_reply":"2023-04-04T06:40:49.548620Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Groupby SibSp to get the mean of survival rate\ntrain_df.groupby('SibSp', as_index=False).Survived.mean().sort_values(by='Survived',ascending=False)","metadata":{"execution":{"iopub.status.busy":"2023-04-04T06:40:49.552439Z","iopub.execute_input":"2023-04-04T06:40:49.552798Z","iopub.status.idle":"2023-04-04T06:40:49.571210Z","shell.execute_reply.started":"2023-04-04T06:40:49.552763Z","shell.execute_reply":"2023-04-04T06:40:49.569630Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looking at SibSp, it seems like the higher number of siblings, the lower the chances of survival. Thus SibSp should also be in the feature selection.","metadata":{}},{"cell_type":"markdown","source":"#### Parch","metadata":{}},{"cell_type":"code","source":"# Plot barchart of Parch vs Survived\nsns.barplot(data=train_df, x='Parch', y='Survived')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-04-04T06:40:49.577834Z","iopub.execute_input":"2023-04-04T06:40:49.579153Z","iopub.status.idle":"2023-04-04T06:40:49.975378Z","shell.execute_reply.started":"2023-04-04T06:40:49.579100Z","shell.execute_reply":"2023-04-04T06:40:49.973932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Group Parch to get the mean of survival rate\ntrain_df.groupby('Parch', as_index=False).Survived.mean().sort_values(by='Survived',ascending=False)","metadata":{"execution":{"iopub.status.busy":"2023-04-04T06:40:49.976937Z","iopub.execute_input":"2023-04-04T06:40:49.977337Z","iopub.status.idle":"2023-04-04T06:40:49.996085Z","shell.execute_reply.started":"2023-04-04T06:40:49.977299Z","shell.execute_reply":"2023-04-04T06:40:49.994611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Generally it seems like there is not much trend in the Parch columns, however, passengers with >0 Parch has a higher survivability which then goes down when the number gets too high","metadata":{}},{"cell_type":"markdown","source":"#### Ticket","metadata":{}},{"cell_type":"markdown","source":"Same as the name column, the ticket column does not provide any useful information that can be used to predict the survivability of the passenger. Thus this column will be dropped.","metadata":{"execution":{"iopub.status.busy":"2023-03-31T09:08:06.385042Z","iopub.execute_input":"2023-03-31T09:08:06.385482Z","iopub.status.idle":"2023-03-31T09:08:06.394397Z","shell.execute_reply.started":"2023-03-31T09:08:06.385439Z","shell.execute_reply":"2023-03-31T09:08:06.393033Z"}}},{"cell_type":"markdown","source":"#### Fare","metadata":{}},{"cell_type":"code","source":"# Plot histogram of Fare with Survived as hue\nsns.histplot(data=train_df, x='Fare', hue='Survived')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-04-04T06:40:49.998799Z","iopub.execute_input":"2023-04-04T06:40:49.999228Z","iopub.status.idle":"2023-04-04T06:40:50.733586Z","shell.execute_reply.started":"2023-04-04T06:40:49.999188Z","shell.execute_reply":"2023-04-04T06:40:50.732266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The Fare histogram indicates that passengers who paid a higher fare had a better chance of surviving compared to those who paid a lower fare. This correlation is likely linked to the Pclass of the tickets, as previously shown in the plot, where a higher Pclass is associated with a greater likelihood of survival.","metadata":{}},{"cell_type":"markdown","source":"#### Cabin\n\nCabin is the different passengers room assigned. It could be useful to use as a feature, however, cabin has 687 missing rows which can be difficult to impute and eventually lead to inaccuracy. Thus cabin will be dropped as well.","metadata":{}},{"cell_type":"markdown","source":"#### Embarked\n\nThere are 2 missing entry of Embarked, **Stone, Mrs. George Nelson (Martha Evelyn)** and **Amelie Icard**. With a simple google search, I was able to find their port of embark which is Southampton. \n\n\"Mrs Stone boarded the Titanic in Southampton on 10 April 1912 and was travelling in first class with her maid Amelie Icard. She occupied cabin B-28.\"","metadata":{}},{"cell_type":"code","source":"# Locate NaN entry for embarked\ntrain_df.loc[train_df['Embarked'].isnull()]","metadata":{"execution":{"iopub.status.busy":"2023-04-04T06:40:50.735382Z","iopub.execute_input":"2023-04-04T06:40:50.735884Z","iopub.status.idle":"2023-04-04T06:40:50.756161Z","shell.execute_reply.started":"2023-04-04T06:40:50.735829Z","shell.execute_reply":"2023-04-04T06:40:50.754812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fill in Southampton as their embarked location\ntrain_df['Embarked'] = train_df['Embarked'].fillna('S')","metadata":{"execution":{"iopub.status.busy":"2023-04-04T06:40:50.757690Z","iopub.execute_input":"2023-04-04T06:40:50.758485Z","iopub.status.idle":"2023-04-04T06:40:50.768323Z","shell.execute_reply.started":"2023-04-04T06:40:50.758424Z","shell.execute_reply":"2023-04-04T06:40:50.766951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot barchart of Embarked vs Survived\nsns.barplot(data=train_df, x='Embarked', y='Survived')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-04-04T06:40:50.769740Z","iopub.execute_input":"2023-04-04T06:40:50.770153Z","iopub.status.idle":"2023-04-04T06:40:51.082238Z","shell.execute_reply.started":"2023-04-04T06:40:50.770112Z","shell.execute_reply":"2023-04-04T06:40:51.080321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Group by Embarked to get the mean of survival rate\ntrain_df.groupby('Embarked',as_index=False).Survived.mean()","metadata":{"execution":{"iopub.status.busy":"2023-04-04T06:40:51.084048Z","iopub.execute_input":"2023-04-04T06:40:51.084422Z","iopub.status.idle":"2023-04-04T06:40:51.103455Z","shell.execute_reply.started":"2023-04-04T06:40:51.084388Z","shell.execute_reply":"2023-04-04T06:40:51.102029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the plot, it seems that passengers who embarked from Cherbourgh has a higher survival rate. Thus this will be also used as a feature.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"subsection-four\"></a>\n### Transforming data\n\nThe different types of features can be categorized into two main groups: **categorical** and **continuous**.\\\nCategorical features are variables that have two or more categories. When these variables cannot be sorted or ordered, it is known as **nominal** variables. Examples of nominal features in the dataset include sex and embarked. However, when they can be sorted or ordered based on their relative values, they are known as **ordinal** features. For instance, the height feature with values tall, medium, and short is an ordinal variable because it can be sorted based on relative height. An example of an ordinal feature in the dataset is PClass.\n\nFinally, continuous features are those that can take any value between the minimum and maximum values of the feature column. These features are referred to as continuous because they can take on an infinite number of values within a given range. An example of continuous feature in this dataset is Fare.","metadata":{}},{"cell_type":"code","source":"# Onehotencode nominal features for train_df\n\n# Create an instance of the encoder\nonehot_encoder = OneHotEncoder()\n\n# Fit the encoder to the 'Sex' column and transform the data\nsex_onehot = onehot_encoder.fit_transform(train_df['Sex'].values.reshape(-1,1)).toarray()\nsex_df = pd.DataFrame(sex_onehot, columns=['female', 'male'])\ntrain_df = pd.concat([train_df, sex_df], axis=1)\n\n# Fit the encoder to the 'Embarked' column and transform the data\nembarked_onehot = onehot_encoder.fit_transform(train_df['Embarked'].values.reshape(-1,1)).toarray()\nembarked_df = pd.DataFrame(embarked_onehot, columns=['C', 'Q', 'S'])\ntrain_df = pd.concat([train_df, embarked_df], axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-04-04T06:40:51.104929Z","iopub.execute_input":"2023-04-04T06:40:51.105311Z","iopub.status.idle":"2023-04-04T06:40:51.121230Z","shell.execute_reply.started":"2023-04-04T06:40:51.105275Z","shell.execute_reply":"2023-04-04T06:40:51.119660Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Onehotencode nominal features for test_df\n# Fit the encoder to the 'Sex' column and transform the data\nsex_onehot = onehot_encoder.fit_transform(test_df['Sex'].values.reshape(-1,1)).toarray()\nsex_df = pd.DataFrame(sex_onehot, columns=['female', 'male'])\ntest_df = pd.concat([test_df, sex_df], axis=1)\n\n# Fit the encoder to the 'Embarked' column and transform the data\nembarked_onehot = onehot_encoder.fit_transform(test_df['Embarked'].values.reshape(-1,1)).toarray()\nembarked_df = pd.DataFrame(embarked_onehot, columns=['C', 'Q', 'S'])\ntest_df = pd.concat([test_df, embarked_df], axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-04-04T06:40:51.122568Z","iopub.execute_input":"2023-04-04T06:40:51.122909Z","iopub.status.idle":"2023-04-04T06:40:51.139154Z","shell.execute_reply.started":"2023-04-04T06:40:51.122877Z","shell.execute_reply":"2023-04-04T06:40:51.137574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dropping not related features\ncols_to_drop = ['PassengerId','Name','Ticket','Cabin','Initial','Sex','Embarked',]\ntrain_df = train_df.drop(columns=cols_to_drop)\ntest_df = test_df.drop(columns=cols_to_drop)","metadata":{"execution":{"iopub.status.busy":"2023-04-04T06:40:51.141638Z","iopub.execute_input":"2023-04-04T06:40:51.142206Z","iopub.status.idle":"2023-04-04T06:40:51.155434Z","shell.execute_reply.started":"2023-04-04T06:40:51.142151Z","shell.execute_reply":"2023-04-04T06:40:51.153663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lets take a final look at our data before going on to modelling","metadata":{}},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-04-04T06:40:51.157330Z","iopub.execute_input":"2023-04-04T06:40:51.158100Z","iopub.status.idle":"2023-04-04T06:40:51.178757Z","shell.execute_reply.started":"2023-04-04T06:40:51.158058Z","shell.execute_reply":"2023-04-04T06:40:51.177286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"section-three\"></a>\n## Modelling and predicting\n\nHaving completed our exploratory data analysis, we now turn our attention to building a predictive model to help us understand the relationships between various features and the likelihood of survival on the Titanic. Our goal is to identify the best algorithm that can classify the passengers into survivors and non-survivors with high accuracy, using the insights gained from the EDA. \n\nWe have selected several supervised learning algorithms to evaluate their performance in solving this classification problem as follows:\n\n1) Logistic regression\n2) k-Nearest Neighbors\n3) Support vector machine\n4) Decision tree \n5) Random forest\n6) XGBoost\n\nBy leveraging the strengths of these algorithms and fine-tuning their parameters, we aim to build a model that can accurately predict the survival of passengers on the Titanic.","metadata":{}},{"cell_type":"code","source":"# Splitting train_df to train test split\nX_train, X_test, y_train, y_test = train_test_split(train_df.drop(columns='Survived'),train_df['Survived'],test_size=0.2,random_state=50)\n\n# Scaling the data\nscaler = StandardScaler()\nscaler.fit(X_train)\n\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)\ntest_df = scaler.transform(test_df)","metadata":{"execution":{"iopub.status.busy":"2023-04-04T06:40:51.180353Z","iopub.execute_input":"2023-04-04T06:40:51.180714Z","iopub.status.idle":"2023-04-04T06:40:51.209501Z","shell.execute_reply.started":"2023-04-04T06:40:51.180680Z","shell.execute_reply":"2023-04-04T06:40:51.208001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fit logistic regression model and predict\nlrmodel = LogisticRegression(max_iter=500)\nlrmodel.fit(X_train,y_train)\nlr_pred = lrmodel.predict(X_test)\n\n# Accuracy of logistic regression model \nlr_acc = accuracy_score(y_test,lr_pred)\nprint(\"Accuracy for logistic regression:\", lr_acc)","metadata":{"execution":{"iopub.status.busy":"2023-04-04T06:40:51.228191Z","iopub.execute_input":"2023-04-04T06:40:51.228620Z","iopub.status.idle":"2023-04-04T06:40:51.242750Z","shell.execute_reply.started":"2023-04-04T06:40:51.228582Z","shell.execute_reply":"2023-04-04T06:40:51.241009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fit k_Nearest Neighbors and predict for different n neighbors\nknn_index=list(range(1,11))\nknn=pd.Series(dtype=float)\nx=[0,1,2,3,4,5,6,7,8,9,10]\nfor i in list(range(1,11)):\n    knnmodel=KNeighborsClassifier(n_neighbors=i) \n    knnmodel.fit(X_train,y_train)\n    knn_pred=knnmodel.predict(X_test)\n    knn=knn.append(pd.Series(accuracy_score(knn_pred,y_test)))\n\nplt.figure(figsize=(12,6))\nplt.plot(knn_index, knn)\nplt.xticks(x)\nplt.title(\"kNN model for different n\")\nplt.show()\nprint('Accuracy for best n kNN model:' ,knn.values.max())","metadata":{"execution":{"iopub.status.busy":"2023-04-04T06:40:51.244762Z","iopub.execute_input":"2023-04-04T06:40:51.245179Z","iopub.status.idle":"2023-04-04T06:40:51.612957Z","shell.execute_reply.started":"2023-04-04T06:40:51.245140Z","shell.execute_reply":"2023-04-04T06:40:51.611534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fit support vector machine and predict\n\n# Linear SVM\nlsvm = LinearSVC(max_iter=5000)\nlsvm.fit(X_train,y_train)\nlsvm_pred = lsvm.predict(X_test)\n\n# RBF SVM\nrbfsvm = SVC(kernel=\"rbf\")\nrbfsvm.fit(X_train,y_train)\nrbfsvm_pred = rbfsvm.predict(X_test)\n\n# Polynomial SVM\npolysvm = SVC(kernel=\"poly\")\npolysvm.fit(X_train,y_train)\npolysvm_pred = polysvm.predict(X_test)\n\n# Accuracy for SVMs\nlsvm_acc = accuracy_score(y_test,lsvm_pred)\nrbfsvm_acc = accuracy_score(y_test,rbfsvm_pred)\npolysvm_acc = accuracy_score(y_test,polysvm_pred)\n\nprint(\"Accuracy for Linear SVM:\", lsvm_acc)\nprint(\"Accuracy for RBF SVM:\", rbfsvm_acc)\nprint(\"Accuracy for Polynomial SVM:\", polysvm_acc)","metadata":{"execution":{"iopub.status.busy":"2023-04-04T06:40:51.615374Z","iopub.execute_input":"2023-04-04T06:40:51.615819Z","iopub.status.idle":"2023-04-04T06:40:51.769802Z","shell.execute_reply.started":"2023-04-04T06:40:51.615773Z","shell.execute_reply":"2023-04-04T06:40:51.768669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Decision tree\ntreemodel = DecisionTreeClassifier()\ntreemodel.fit(X_train,y_train)\ntreemodel_pred = treemodel.predict(X_test)\n\n# Accuracy of Decision tree\ntreemodel_acc = accuracy_score(y_test,treemodel_pred)\nprint(\"Accuracy of Decision tree:\", treemodel_acc)","metadata":{"execution":{"iopub.status.busy":"2023-04-04T06:40:51.770950Z","iopub.execute_input":"2023-04-04T06:40:51.771349Z","iopub.status.idle":"2023-04-04T06:40:51.785150Z","shell.execute_reply.started":"2023-04-04T06:40:51.771312Z","shell.execute_reply":"2023-04-04T06:40:51.783869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Random forest\nforestmodel = RandomForestClassifier()\nforestmodel.fit(X_train,y_train)\nforestmodel_pred = forestmodel.predict(X_test)\n\n# Accuracy of Random forest\nforestmodel_acc = accuracy_score(y_test,forestmodel_pred)\nprint(\"Accuracy of Random forest:\", forestmodel_acc)","metadata":{"execution":{"iopub.status.busy":"2023-04-04T06:40:51.786986Z","iopub.execute_input":"2023-04-04T06:40:51.787438Z","iopub.status.idle":"2023-04-04T06:40:52.025651Z","shell.execute_reply.started":"2023-04-04T06:40:51.787315Z","shell.execute_reply":"2023-04-04T06:40:52.024002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# XGBoost\nXGBmodel = XGBClassifier(n_estimators=100, early_stopping_rounds=5)\nXGBmodel.fit(X_train,y_train, eval_set=[(X_test, y_test)],verbose=False)\nXGBmodel_pred = XGBmodel.predict(X_test)\n\n# Accuracy of XGBoost\nXGBmodel_acc = accuracy_score(y_test,XGBmodel_pred)\nprint(\"Accuracy of XGBoost:\", XGBmodel_acc)\n","metadata":{"execution":{"iopub.status.busy":"2023-04-04T06:40:52.027098Z","iopub.execute_input":"2023-04-04T06:40:52.027438Z","iopub.status.idle":"2023-04-04T06:40:52.176149Z","shell.execute_reply.started":"2023-04-04T06:40:52.027405Z","shell.execute_reply":"2023-04-04T06:40:52.174389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"subsection-five\"></a>\n### Cross Validation\n\nCross-validation (CV) is a technique used in machine learning to evaluate the performance of a model. In simple modelling, we typically split our dataset into training and testing sets and evaluate the model's performance on the testing set. However, this approach has some limitations, such as a high variance in the evaluation metric, which could lead to overfitting or underfitting of the model. Cross-validation provides a more robust and reliable way to assess model performance by splitting the data into multiple folds and training the model on different subsets of the data. This allows us to obtain a more stable estimate of the model's performance and reduce the risk of overfitting or underfitting. In this way, cross-validation helps us to ensure that our model is generalizable and performs well on new, unseen data.\n","metadata":{}},{"cell_type":"code","source":"# Define the models to be tested\nmodels = {\n    \"Logistic Regression\": LogisticRegression(),\n    \"Random Forest\": RandomForestClassifier(),\n    \"XGBoost\": XGBClassifier(),\n    \"KNN\": KNeighborsClassifier(),\n    \"RBF SVM\": SVC(kernel='rbf'),\n    \"Decision Tree\": DecisionTreeClassifier()\n}\n\n# Define the hyperparameters to be tested for each model\nparams = {\n    \"Logistic Regression\": {\n        \"C\": [0.1, 1.0, 10.0],\n        \"solver\": [\"lbfgs\", \"liblinear\"]\n    },\n    \"Random Forest\": {\n        \"n_estimators\": [10, 50, 100],\n        \"max_depth\": [None, 5, 10],\n        \"min_samples_split\": [2, 5, 10]\n    },\n    \"XGBoost\": {\n        \"learning_rate\": [0.01, 0.1, 0.5],\n        \"n_estimators\": [50, 100, 200],\n        \"max_depth\": [3, 5, 7],\n        \"subsample\": [0.5, 0.7, 1.0],\n        \"colsample_bytree\": [0.5, 0.7, 1.0]\n    },\n    \"KNN\": {\n        \"n_neighbors\": [2, 4, 6, 8, 10],\n        \"weights\": [\"uniform\", \"distance\"]\n    },\n    \"RBF SVM\": {\n        \"C\": [0.1, 1.0, 10.0],\n        \"gamma\": [0.01, 0.1, 1.0]\n    },\n    \"Decision Tree\": {\n        \"max_depth\": [None, 5, 10],\n        \"min_samples_split\": [2, 5, 10],\n        \"min_samples_leaf\": [1, 2, 4]\n    }\n}\n\n# Define the number of folds for cross-validation\nk = 5\ncv = KFold(n_splits=k, shuffle=True, random_state=42)\n\n# Perform grid search for each model\nfor model_name, model in models.items():\n    print(f\"Performing grid search for {model_name}...\")\n    grid_search = GridSearchCV(\n        model,\n        params[model_name],\n        cv=cv,\n        scoring=\"accuracy\",\n        n_jobs=-1\n    )\n    grid_search.fit(X_train, y_train)\n    print(f\"Best parameters: {grid_search.best_params_}\")\n    print(f\"Training accuracy: {grid_search.best_score_:.3f}\")\n    print(f\"Test accuracy: {grid_search.score(X_test, y_test):.3f}\")\n","metadata":{"execution":{"iopub.status.busy":"2023-04-04T06:44:00.194126Z","iopub.execute_input":"2023-04-04T06:44:00.194553Z","iopub.status.idle":"2023-04-04T06:47:54.237589Z","shell.execute_reply.started":"2023-04-04T06:44:00.194516Z","shell.execute_reply":"2023-04-04T06:47:54.236368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"section-four\"></a>\n## Submission","metadata":{}},{"cell_type":"code","source":"# Predictions using XGBoost for test_df and output for submission\n\n#Train the XGBoost model with the best parameters\nmodel = XGBClassifier(\n    learning_rate=0.1,\n    max_depth=5,\n    n_estimators=100,\n    subsample=0.7,\n    colsample_bytree=1.0)\n\nmodel.fit(X_train, y_train)\n\npredictions =  model.predict(test_df)\ntest_data = pd.read_csv('/kaggle/input/titanic/test.csv')\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","metadata":{"execution":{"iopub.status.busy":"2023-04-04T06:48:51.717887Z","iopub.execute_input":"2023-04-04T06:48:51.718368Z","iopub.status.idle":"2023-04-04T06:48:52.203776Z","shell.execute_reply.started":"2023-04-04T06:48:51.718325Z","shell.execute_reply":"2023-04-04T06:48:52.202711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"section-five\"></a>\n## Conclusion\n\nFor this Titanic dataset, I started by performing exploratory data analysis using Seaborn to visualize various features and their relationship with the target variable. Through this process, I was able to identify several features that seemed to have a significant impact on survival rate, including sex, class, and age. I also filled in missing values for age and embarked location using appropriate methods.\n\nNext, I selected relevant features for the model based on the EDA. For categorical variables, I one-hot encoded nominal variables and left ordinal variables untouched. I then used three classification models - logistic regression, random forest, and XGBoost - as well as KNN, RBF SVM, and decision tree, and performed grid search cross-validation to find the best hyperparameters for each model.\n\nFrom the results, the XGBoost model performed the best with an accuracy of 0.821. Additionally, I was able to obtain a better understanding of the importance of each feature in predicting survival rate, with age, sex, and class being the most important features.\n\nOverall, this project provided me with valuable experience in exploring and preparing data, selecting relevant features, and building and optimizing machine learning models.","metadata":{}}]}